package main

import (
	"context"
	"encoding/json"
	"flag"
	"fmt"
	"log"
	"net/http"
	"os"
	"os/signal"
	"path/filepath"
	"strings"
	"syscall"
	"time"

	"github.com/AgoraIO/RTC-Egress/pkg/uploader"
	"github.com/AgoraIO/RTC-Egress/pkg/utils"
	"github.com/AgoraIO/RTC-Egress/pkg/version"
	"github.com/gin-gonic/gin"
	"github.com/spf13/viper"
)

// Config matches uploader_config.yaml.example structure
type Config struct {
	Server struct {
		HealthPort int `mapstructure:"health_port"`
	} `mapstructure:"server"`
	Pod struct {
		Region string `mapstructure:"region"`
	} `mapstructure:"pod"`
	Redis struct {
		Addr     string `mapstructure:"addr"`
		Password string `mapstructure:"password"`
		DB       int    `mapstructure:"db"`
	} `mapstructure:"redis"`
	Watcher struct {
		Directories    []string `mapstructure:"directories"`
		PollInterval   int      `mapstructure:"poll_interval"`
		RetryInterval  int      `mapstructure:"retry_interval"`
		MaxRetries     int      `mapstructure:"max_retries"`
		DeleteAfter    bool     `mapstructure:"delete_after"`
		MinFileSize    int64    `mapstructure:"min_file_size"`
		MaxFileAge     int      `mapstructure:"max_file_age"`
		FileExtensions []string `mapstructure:"file_extensions"`
	} `mapstructure:"watcher"`
	S3          uploader.S3Config `mapstructure:"s3"`
	Concurrency struct {
		UploadTimeout int `mapstructure:"upload_timeout"`
		MaxWorkers    int `mapstructure:"max_workers"`
		QueueSize     int `mapstructure:"queue_size"`
		BatchSize     int `mapstructure:"batch_size"`
		BatchTimeout  int `mapstructure:"batch_timeout"`
	} `mapstructure:"concurrency"`
}

// MetadataFile represents the structure of metadata JSON files generated by C++ workers
type MetadataFile struct {
	TaskID      string     `json:"taskId"`
	ChannelName string     `json:"channelName"`
	Files       []FileInfo `json:"files"`
	CompletedAt *time.Time `json:"completedAt,omitempty"`
}

// FileInfo represents individual files in the metadata
type FileInfo struct {
	Filename          string     `json:"filename"`
	Path              string     `json:"path"`
	Size              int64      `json:"size"`
	CompletedAt       *time.Time `json:"completedAt,omitempty"`
	UploadStatus      string     `json:"uploadStatus,omitempty"` // pending, uploaded, failed
	UploadURL         string     `json:"uploadUrl,omitempty"`    // S3 URL after successful upload
	UploadAttempts    int        `json:"uploadAttempts,omitempty"`
	LastUploadError   string     `json:"lastUploadError,omitempty"`
	UploadedAt        *time.Time `json:"uploadedAt,omitempty"`
	DeleteAfterUpload bool       `json:"deleteAfterUpload,omitempty"`
}

var (
	config    Config
	watchers  []*uploader.Watcher
	startTime = time.Now()
)

func loadConfig() error {
	// Support CLI flag and use shared resolver
	var cfFlag string
	flag.StringVar(&cfFlag, "config", "", "Path to config YAML file")
	_ = flag.CommandLine.Parse(os.Args[1:])
	resolved, err := utils.ResolveConfigFile("uploader_config.yaml", os.Args[1:])
	if err != nil {
		log.Fatal(err)
	}
	log.Printf("Using config file: %s", resolved)
	viper.SetConfigFile(resolved)
	viper.SetConfigType("yaml")

	// Enable environment variable overrides
	viper.AutomaticEnv()
	viper.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))

	// Set environment variable bindings
	// Set environment variable bindings
	viper.BindEnv("pod.region", "POD_REGION")
	viper.BindEnv("s3.bucket", "S3_BUCKET")
	viper.BindEnv("s3.region", "S3_REGION")
	viper.BindEnv("s3.access_key", "S3_ACCESS_KEY")
	viper.BindEnv("s3.secret_key", "S3_SECRET_KEY")
	viper.BindEnv("s3.endpoint", "S3_ENDPOINT")

	// Read config file
	if err := viper.ReadInConfig(); err != nil {
		return fmt.Errorf("error reading config file: %v", err)
	}

	// Unmarshal the entire config
	if err := viper.Unmarshal(&config); err != nil {
		return fmt.Errorf("error unmarshaling config: %v", err)
	}

	// Debug: Check viper values before validation
	log.Printf("Viper raw values:")
	log.Printf("  s3.access_key: '%s'", viper.GetString("s3.access_key"))
	log.Printf("  s3.secret_key length: %d", len(viper.GetString("s3.secret_key")))

	// Debug: Log S3 config values (without sensitive data)
	log.Printf("Uploader configuration loaded:")
	log.Printf("  S3 Bucket: %s (region: %s)", config.S3.Bucket, config.S3.Region)
	log.Printf("  S3 Access Key: %s (length: %d)",
		func() string {
			if config.S3.AccessKey == "" {
				return "EMPTY"
			}
			return config.S3.AccessKey[:min(8, len(config.S3.AccessKey))] + "..."
		}(), len(config.S3.AccessKey))

	// Validate mandatory fields
	if config.S3.Bucket == "" {
		return fmt.Errorf("s3.bucket is required for uploader service")
	}
	if config.S3.Region == "" {
		return fmt.Errorf("s3.region is required for uploader service")
	}
	if len(config.Watcher.Directories) == 0 {
		return fmt.Errorf("watcher.directories is required - specify directories to watch")
	}

	// Set defaults
	if config.Server.HealthPort <= 0 {
		config.Server.HealthPort = 8185 // Different from other services
	}

	log.Printf("Uploader configuration loaded:")
	log.Printf("  S3 Bucket: %s (region: %s)", config.S3.Bucket, config.S3.Region)
	log.Printf("  Watch Directories: %v", config.Watcher.Directories)

	return nil
}

// findFileInMetadata finds a file's metadata and returns the metadata path and file info
func findFileInMetadata(filePath string) (string, *FileInfo) {
	// Skip metadata files themselves
	if strings.HasSuffix(filePath, ".json") {
		return "", nil
	}

	dir := filepath.Dir(filePath)
	filename := filepath.Base(filePath)

	// Scan directory for .json files
	metadataFiles, err := filepath.Glob(filepath.Join(dir, "*.json"))
	if err != nil {
		return "", nil
	}

	for _, metadataPath := range metadataFiles {
		if fileInfo := findFileInMetadataFile(metadataPath, filename); fileInfo != nil {
			// Only return files that are completed
			if fileInfo.CompletedAt != nil {
				return metadataPath, fileInfo
			}
		}
	}

	return "", nil
}

// findFileInMetadataFile checks if a file is listed in a specific metadata file
func findFileInMetadataFile(metadataPath, targetFilename string) *FileInfo {
	data, err := os.ReadFile(metadataPath)
	if err != nil {
		return nil
	}

	var metadata MetadataFile
	if err := json.Unmarshal(data, &metadata); err != nil {
		return nil
	}

	// Find matching file in the metadata
	for i, fileInfo := range metadata.Files {
		if filepath.Base(fileInfo.Filename) == targetFilename ||
			filepath.Base(fileInfo.Path) == targetFilename {
			return &metadata.Files[i]
		}
	}

	return nil
}

// updateMetadataFile updates the metadata file with upload status
func updateMetadataFile(metadataPath string, fileInfo *FileInfo, operation, details string) {
	// Read current metadata
	data, err := os.ReadFile(metadataPath)
	if err != nil {
		log.Printf("Failed to read metadata file %s: %v", metadataPath, err)
		return
	}

	var metadata MetadataFile
	if err := json.Unmarshal(data, &metadata); err != nil {
		log.Printf("Failed to parse metadata file %s: %v", metadataPath, err)
		return
	}

	// Find and update the specific file info
	updated := false
	for i := range metadata.Files {
		if (metadata.Files[i].Filename == fileInfo.Filename && fileInfo.Filename != "") ||
			(metadata.Files[i].Path == fileInfo.Path && fileInfo.Path != "") {
			metadata.Files[i] = *fileInfo
			updated = true
			break
		}
	}

	if !updated {
		log.Printf("Warning: Could not find matching file in metadata to update")
		return
	}

	// Write back to file
	updatedData, err := json.MarshalIndent(metadata, "", "  ")
	if err != nil {
		log.Printf("Failed to marshal updated metadata: %v", err)
		return
	}

	if err := os.WriteFile(metadataPath, updatedData, 0644); err != nil {
		log.Printf("Failed to write updated metadata to %s: %v", metadataPath, err)
		return
	}

	log.Printf("Updated metadata %s: %s - %s", metadataPath, operation, details)
}

func startFileWatchers() error {
	if config.S3.Bucket == "" {
		return fmt.Errorf("S3 bucket not configured, cannot start file watchers")
	}

	// Create a custom uploader with completion checking
	customUploader, err := NewCompletionAwareUploader(config.S3)
	if err != nil {
		return fmt.Errorf("failed to create completion-aware uploader: %v", err)
	}

	watchers = make([]*uploader.Watcher, 0, len(config.Watcher.Directories))

	for _, watchDir := range config.Watcher.Directories {
		log.Printf("Creating file watcher for directory: %s", watchDir)

		// Create watcher with custom uploader logic
		watcher, err := NewCustomWatcher(watchDir, config.S3, customUploader)
		if err != nil {
			log.Printf("Failed to create file watcher for %s: %v", watchDir, err)
			continue
		}

		watchers = append(watchers, watcher)

		// Start each watcher in a separate goroutine
		go func(watchDir string, w *uploader.Watcher) {
			log.Printf("Starting file watcher for directory: %s", watchDir)
			if err := w.Start(context.Background()); err != nil {
				log.Printf("File watcher error for %s: %v", watchDir, err)
			} else {
				log.Printf("File watcher for %s started successfully", watchDir)
			}
		}(watchDir, watcher)
	}

	if len(watchers) == 0 {
		return fmt.Errorf("no file watchers started successfully")
	}

	log.Printf("Started %d file watchers", len(watchers))
	return nil
}

// CompletionAwareUploader wraps the S3 uploader with completion checking
type CompletionAwareUploader struct {
	uploader *uploader.S3Uploader
}

func NewCompletionAwareUploader(s3Config uploader.S3Config) (*CompletionAwareUploader, error) {
	u, err := uploader.NewS3Uploader(s3Config)
	if err != nil {
		return nil, err
	}
	return &CompletionAwareUploader{uploader: u}, nil
}

func (u *CompletionAwareUploader) UploadFile(filePath string) error {
	// Only upload files that are from completed tasks
	metadataPath, fileInfo := findFileInMetadata(filePath)
	if metadataPath == "" || fileInfo == nil {
		log.Printf("Skipping upload of %s - no metadata found or file not completed", filePath)
		return nil
	}

	// Check if already uploaded
	if fileInfo.UploadStatus == "uploaded" {
		log.Printf("Skipping %s - already uploaded to %s", filePath, fileInfo.UploadURL)
		return nil
	}

	// Attempt upload with retry logic
	return u.uploadWithRetry(filePath, metadataPath, fileInfo)
}

func (u *CompletionAwareUploader) uploadWithRetry(filePath, metadataPath string, fileInfo *FileInfo) error {
	maxRetries := 3
	baseDelay := 5 * time.Second

	for attempt := 1; attempt <= maxRetries; attempt++ {
		log.Printf("Upload attempt %d/%d for file: %s", attempt, maxRetries, filePath)

		// Update upload attempt count
		fileInfo.UploadAttempts = attempt
		updateMetadataFile(metadataPath, fileInfo, "upload_attempted", "")

		err := u.uploader.UploadFile(filePath)
		if err == nil {
			// Upload successful - update metadata and delete file
			log.Printf("Successfully uploaded file: %s", filePath)

			// Generate S3 URL (simplified - in real implementation you'd get this from the upload result)
			filename := filepath.Base(filePath)
			s3URL := fmt.Sprintf("https://%s.s3.amazonaws.com/%s", config.S3.Bucket, filename)

			uploadedAt := time.Now()
			fileInfo.UploadStatus = "uploaded"
			fileInfo.UploadURL = s3URL
			fileInfo.UploadedAt = &uploadedAt
			fileInfo.LastUploadError = ""

			// Update metadata with success
			updateMetadataFile(metadataPath, fileInfo, "upload_completed", s3URL)

			// Delete local file if configured (defaulting to true for now)
			deleteAfterUpload := true // This could be configurable per task
			if deleteAfterUpload {
				if err := os.Remove(filePath); err != nil {
					log.Printf("Warning: Failed to delete file %s after upload: %v", filePath, err)
				} else {
					log.Printf("Deleted local file after successful upload: %s", filePath)
					updateMetadataFile(metadataPath, fileInfo, "file_deleted_after_upload", "")
				}
			}

			return nil
		}

		// Upload failed - update metadata with error
		log.Printf("Upload attempt %d failed for %s: %v", attempt, filePath, err)
		fileInfo.LastUploadError = err.Error()
		updateMetadataFile(metadataPath, fileInfo, "upload_failed", err.Error())

		// If not the last attempt, wait with exponential backoff
		if attempt < maxRetries {
			delay := time.Duration(attempt) * baseDelay
			log.Printf("Waiting %v before retry %d", delay, attempt+1)
			time.Sleep(delay)
		}
	}

	// All retries failed
	fileInfo.UploadStatus = "failed"
	updateMetadataFile(metadataPath, fileInfo, "upload_failed_final", "Max retries exceeded")
	return fmt.Errorf("upload failed after %d attempts", maxRetries)
}

// NewCustomWatcher creates a watcher that uses completion-aware uploading
func NewCustomWatcher(outputDir string, s3Config uploader.S3Config, customUploader *CompletionAwareUploader) (*uploader.Watcher, error) {
	// We'll use the existing watcher structure but need to adapt it
	// For now, let's use the standard watcher - the completion check happens in the uploader
	return uploader.NewWatcher(outputDir, s3Config)
}

func healthCheckHandler(c *gin.Context) {
	// Simple health check using existing watcher information
	status := "healthy"
	if len(watchers) == 0 {
		status = "unhealthy"
	}

	watcherInfo := make([]map[string]string, len(watchers))
	for i := range watchers {
		watcherInfo[i] = map[string]string{
			"directory": config.Watcher.Directories[i],
			"status":    "active",
		}
	}

	c.JSON(http.StatusOK, gin.H{
		"status":            status,
		"service":           "uploader",
		"version":           version.GetVersion(),
		"uptime":            time.Since(startTime).String(),
		"watchers":          watcherInfo,
		"s3_bucket":         config.S3.Bucket,
		"s3_region":         config.S3.Region,
		"watch_directories": config.Watcher.Directories,
	})
}

func startHealthCheckServer() {
	r := gin.New()
	r.Use(utils.MyCustomLogger(), gin.Recovery())

	// Health check
	r.GET("/health", healthCheckHandler)

	// Simple metrics endpoint
	r.GET("/metrics", func(c *gin.Context) {
		metrics := map[string]interface{}{
			"watchers_count": len(watchers),
			"directories":    config.Watcher.Directories,
			"s3_bucket":      config.S3.Bucket,
			"uptime":         time.Since(startTime).String(),
		}
		c.JSON(http.StatusOK, metrics)
	})

	go func() {
		srv := &http.Server{
			Addr:    fmt.Sprintf(":%d", config.Server.HealthPort),
			Handler: r,
		}
		if err := srv.ListenAndServe(); err != nil && err != http.ErrServerClosed {
			log.Fatalf("Failed to start health check server: %v", err)
		}
	}()

	log.Printf("Uploader health server started on port %d", config.Server.HealthPort)
}

func scanAndUploadExistingFiles() {
	// Periodically scan for newly completed files that might have been missed
	ticker := time.NewTicker(60 * time.Second) // Increased to 60 seconds to be less aggressive
	defer ticker.Stop()

	customUploader, err := NewCompletionAwareUploader(config.S3)
	if err != nil {
		log.Printf("Failed to create uploader for background scanning: %v", err)
		return
	}

	for range ticker.C {
		log.Printf("Scanning for completed files to upload...")
		for _, watchDir := range config.Watcher.Directories {
			filepath.Walk(watchDir, func(path string, info os.FileInfo, err error) error {
				if err != nil {
					return nil
				}

				// Skip directories and metadata files
				if info.IsDir() || strings.HasSuffix(path, ".json") {
					return nil
				}

				// Use the completion-aware uploader which handles all the logic
				if err := customUploader.UploadFile(path); err != nil {
					log.Printf("Background scan: Failed to process file %s: %v", path, err)
				}

				return nil
			})
		}
	}
}

func main() {
	// Load configuration
	if err := loadConfig(); err != nil {
		log.Fatalf("Failed to load configuration: %v", err)
	}

	// Start health check server
	startHealthCheckServer()

	// Start file watchers (main functionality)
	if err := startFileWatchers(); err != nil {
		log.Fatalf("Failed to start file watchers: %v", err)
	}

	// Start background scanner for missed completed files
	go scanAndUploadExistingFiles()

	log.Printf("Uploader service started successfully")
	log.Printf("Watching directories: %v", config.Watcher.Directories)
	log.Printf("Uploading to S3: %s/%s", config.S3.Region, config.S3.Bucket)
	log.Printf("Health endpoint: http://localhost:%d/health", config.Server.HealthPort)

	// Wait for interrupt signal to gracefully shutdown
	quit := make(chan os.Signal, 1)
	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
	<-quit

	log.Println("Uploader service stopping...")

	// The existing watcher package handles graceful shutdown via context cancellation
	log.Printf("Stopped %d watchers", len(watchers))

	log.Println("Uploader service stopped")
}
