package main

import (
	"context"
	"encoding/json"
	"flag"
	"fmt"
	"net/http"
	"os"
	"os/signal"
	"path/filepath"
	"strings"
	"syscall"
	"time"

	"github.com/AgoraIO/RTC-Egress/pkg/logger"
	"github.com/AgoraIO/RTC-Egress/pkg/uploader"
	"github.com/AgoraIO/RTC-Egress/pkg/utils"
	"github.com/AgoraIO/RTC-Egress/pkg/version"
	"github.com/gin-gonic/gin"
	"github.com/spf13/viper"
)

// Config matches uploader_config.yaml.example structure
type Config struct {
	Server struct {
		HealthPort int `mapstructure:"health_port"`
	} `mapstructure:"server"`
	Pod struct {
		Region string `mapstructure:"region"`
	} `mapstructure:"pod"`
	Redis struct {
		Addr     string `mapstructure:"addr"`
		Password string `mapstructure:"password"`
		DB       int    `mapstructure:"db"`
	} `mapstructure:"redis"`
	Watcher struct {
		Directories    []string `mapstructure:"directories"`
		PollInterval   int      `mapstructure:"poll_interval"`
		RetryInterval  int      `mapstructure:"retry_interval"`
		MaxRetries     int      `mapstructure:"max_retries"`
		DeleteAfter    bool     `mapstructure:"delete_after"`
		MinFileSize    int64    `mapstructure:"min_file_size"`
		MaxFileAge     int      `mapstructure:"max_file_age"`
		FileExtensions []string `mapstructure:"file_extensions"`
	} `mapstructure:"watcher"`
	S3          uploader.S3Config `mapstructure:"s3"`
	Concurrency struct {
		UploadTimeout int `mapstructure:"upload_timeout"`
		MaxWorkers    int `mapstructure:"max_workers"`
		QueueSize     int `mapstructure:"queue_size"`
		BatchSize     int `mapstructure:"batch_size"`
		BatchTimeout  int `mapstructure:"batch_timeout"`
	} `mapstructure:"concurrency"`
}

// MetadataFile represents the structure of metadata JSON files generated by C++ workers
type MetadataFile struct {
	TaskID      string     `json:"taskId"`
	ChannelName string     `json:"channelName"`
	Files       []FileInfo `json:"files"`
	CompletedAt *time.Time `json:"completedAt,omitempty"`
}

// FileInfo represents individual files in the metadata
type FileInfo struct {
	Filename          string     `json:"filename"`
	Path              string     `json:"path"`
	Size              int64      `json:"size"`
	CompletedAt       *time.Time `json:"completedAt,omitempty"`
	UploadStatus      string     `json:"uploadStatus,omitempty"` // pending, uploaded, failed
	UploadURL         string     `json:"uploadUrl,omitempty"`    // S3 URL after successful upload
	UploadAttempts    int        `json:"uploadAttempts,omitempty"`
	LastUploadError   string     `json:"lastUploadError,omitempty"`
	UploadedAt        *time.Time `json:"uploadedAt,omitempty"`
	DeleteAfterUpload bool       `json:"deleteAfterUpload,omitempty"`
}

var (
	config    Config
	watchers  []*uploader.Watcher
	startTime = time.Now()
)

func maskKey(value string) string {
	if value == "" {
		return "EMPTY"
	}
	if len(value) <= 6 {
		return value
	}
	return value[:6] + "..."
}

func loadConfig() error {
	// Support CLI flag and use shared resolver
	var cfFlag string
	flag.StringVar(&cfFlag, "config", "", "Path to config YAML file")
	_ = flag.CommandLine.Parse(os.Args[1:])
	resolved, err := utils.ResolveConfigFile("uploader_config.yaml", os.Args[1:])
	if err != nil {
		logger.Fatal("failed to resolve uploader config", logger.ErrorField(err))
	}
	logger.Info("Using config file", logger.String("path", resolved))
	viper.SetConfigFile(resolved)
	viper.SetConfigType("yaml")

	// Enable environment variable overrides
	viper.AutomaticEnv()
	viper.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))

	// Set environment variable bindings
	// Set environment variable bindings
	viper.BindEnv("pod.region", "POD_REGION")
	viper.BindEnv("s3.bucket", "S3_BUCKET")
	viper.BindEnv("s3.region", "S3_REGION")
	viper.BindEnv("s3.access_key", "S3_ACCESS_KEY")
	viper.BindEnv("s3.secret_key", "S3_SECRET_KEY")
	viper.BindEnv("s3.endpoint", "S3_ENDPOINT")

	// Read config file
	if err := viper.ReadInConfig(); err != nil {
		return fmt.Errorf("error reading config file: %v", err)
	}

	// Unmarshal the entire config
	if err := viper.Unmarshal(&config); err != nil {
		return fmt.Errorf("error unmarshaling config: %v", err)
	}
	config.Redis.Addr = utils.ResolveRedisAddr(config.Redis.Addr)

	// Debug: Check viper values before validation
	logger.Debug("Uploader Viper raw values",
		logger.String("s3_access_key_prefix", maskKey(viper.GetString("s3.access_key"))),
		logger.Int("s3_secret_key_length", len(viper.GetString("s3.secret_key"))))

	// Log S3 config values (without sensitive data)
	logger.Info("Uploader configuration loaded",
		logger.String("s3_bucket", config.S3.Bucket),
		logger.String("s3_region", config.S3.Region),
		logger.String("s3_endpoint", config.S3.Endpoint))
	logger.Info("Uploader access key summary",
		logger.String("access_key_prefix", maskKey(config.S3.AccessKey)),
		logger.Int("access_key_length", len(config.S3.AccessKey)))

	// Validate mandatory fields
	if config.S3.Bucket == "" {
		return fmt.Errorf("s3.bucket is required for uploader service")
	}
	if config.S3.Region == "" {
		return fmt.Errorf("s3.region is required for uploader service")
	}
	if len(config.Watcher.Directories) == 0 {
		return fmt.Errorf("watcher.directories is required - specify directories to watch")
	}

	// Set defaults
	if config.Server.HealthPort <= 0 {
		config.Server.HealthPort = 8185 // Different from other services
	}

	logger.Info("Uploader watch configuration",
		logger.String("s3_bucket", config.S3.Bucket),
		logger.String("s3_region", config.S3.Region),
		logger.String("watch_directories", fmt.Sprint(config.Watcher.Directories)))

	return nil
}

// findFileInMetadata finds a file's metadata and returns the metadata path and file info
func findFileInMetadata(filePath string) (string, *FileInfo) {
	// Skip metadata files themselves
	if strings.HasSuffix(filePath, ".json") {
		return "", nil
	}

	dir := filepath.Dir(filePath)
	filename := filepath.Base(filePath)

	// Scan directory for .json files
	metadataFiles, err := filepath.Glob(filepath.Join(dir, "*.json"))
	if err != nil {
		return "", nil
	}

	for _, metadataPath := range metadataFiles {
		if fileInfo := findFileInMetadataFile(metadataPath, filename); fileInfo != nil {
			// Only return files that are completed
			if fileInfo.CompletedAt != nil {
				return metadataPath, fileInfo
			}
		}
	}

	return "", nil
}

// findFileInMetadataFile checks if a file is listed in a specific metadata file
func findFileInMetadataFile(metadataPath, targetFilename string) *FileInfo {
	data, err := os.ReadFile(metadataPath)
	if err != nil {
		return nil
	}

	var metadata MetadataFile
	if err := json.Unmarshal(data, &metadata); err != nil {
		return nil
	}

	// Find matching file in the metadata
	for i, fileInfo := range metadata.Files {
		if filepath.Base(fileInfo.Filename) == targetFilename ||
			filepath.Base(fileInfo.Path) == targetFilename {
			return &metadata.Files[i]
		}
	}

	return nil
}

// updateMetadataFile updates the metadata file with upload status
func updateMetadataFile(metadataPath string, fileInfo *FileInfo, operation, details string) {
	// Read current metadata
	data, err := os.ReadFile(metadataPath)
	if err != nil {
		logger.Error("Failed to read metadata file", logger.String("metadata_path", metadataPath), logger.ErrorField(err))
		return
	}

	var metadata MetadataFile
	if err := json.Unmarshal(data, &metadata); err != nil {
		logger.Error("Failed to parse metadata file", logger.String("metadata_path", metadataPath), logger.ErrorField(err))
		return
	}

	// Find and update the specific file info
	updated := false
	for i := range metadata.Files {
		if (metadata.Files[i].Filename == fileInfo.Filename && fileInfo.Filename != "") ||
			(metadata.Files[i].Path == fileInfo.Path && fileInfo.Path != "") {
			metadata.Files[i] = *fileInfo
			updated = true
			break
		}
	}

	if !updated {
		logger.Warn("Could not find matching file in metadata to update", logger.String("metadata_path", metadataPath))
		return
	}

	// Write back to file
	updatedData, err := json.MarshalIndent(metadata, "", "  ")
	if err != nil {
		logger.Error("Failed to marshal updated metadata", logger.String("metadata_path", metadataPath), logger.ErrorField(err))
		return
	}

	if err := os.WriteFile(metadataPath, updatedData, 0644); err != nil {
		logger.Error("Failed to write updated metadata", logger.String("metadata_path", metadataPath), logger.ErrorField(err))
		return
	}

	logger.Info("Updated metadata entry",
		logger.String("metadata_path", metadataPath),
		logger.String("operation", operation),
		logger.String("details", details))
}

func startFileWatchers() error {
	if config.S3.Bucket == "" {
		return fmt.Errorf("S3 bucket not configured, cannot start file watchers")
	}

	// Create a custom uploader with completion checking
	customUploader, err := NewCompletionAwareUploader(config.S3)
	if err != nil {
		return fmt.Errorf("failed to create completion-aware uploader: %v", err)
	}

	watchers = make([]*uploader.Watcher, 0, len(config.Watcher.Directories))

	for _, watchDir := range config.Watcher.Directories {
		logger.Info("Creating file watcher", logger.String("directory", watchDir))

		// Create watcher with custom uploader logic
		watcher, err := NewCustomWatcher(watchDir, config.S3, customUploader)
		if err != nil {
			logger.Error("Failed to create file watcher", logger.String("directory", watchDir), logger.ErrorField(err))
			continue
		}

		watchers = append(watchers, watcher)

		// Start each watcher in a separate goroutine
		go func(watchDir string, w *uploader.Watcher) {
			logger.Info("Starting file watcher", logger.String("directory", watchDir))
			if err := w.Start(context.Background()); err != nil {
				logger.Error("File watcher error", logger.String("directory", watchDir), logger.ErrorField(err))
			} else {
				logger.Info("File watcher started successfully", logger.String("directory", watchDir))
			}
		}(watchDir, watcher)
	}

	if len(watchers) == 0 {
		return fmt.Errorf("no file watchers started successfully")
	}

	logger.Info("File watchers started", logger.Int("count", len(watchers)))
	return nil
}

// CompletionAwareUploader wraps the S3 uploader with completion checking
type CompletionAwareUploader struct {
	uploader *uploader.S3Uploader
}

func NewCompletionAwareUploader(s3Config uploader.S3Config) (*CompletionAwareUploader, error) {
	u, err := uploader.NewS3Uploader(s3Config)
	if err != nil {
		return nil, err
	}
	return &CompletionAwareUploader{uploader: u}, nil
}

func (u *CompletionAwareUploader) UploadFile(filePath string) error {
	// Only upload files that are from completed tasks
	metadataPath, fileInfo := findFileInMetadata(filePath)
	if metadataPath == "" || fileInfo == nil {
		logger.Debug("Skipping upload - metadata missing or incomplete",
			logger.String("file", filePath))
		return nil
	}

	// Check if already uploaded
	if fileInfo.UploadStatus == "uploaded" {
		logger.Info("Skipping already uploaded file",
			logger.String("file", filePath),
			logger.String("upload_url", fileInfo.UploadURL))
		return nil
	}

	// Attempt upload with retry logic
	return u.uploadWithRetry(filePath, metadataPath, fileInfo)
}

func (u *CompletionAwareUploader) uploadWithRetry(filePath, metadataPath string, fileInfo *FileInfo) error {
	maxRetries := 3
	baseDelay := 5 * time.Second

	for attempt := 1; attempt <= maxRetries; attempt++ {
		logger.Info("Uploading file",
			logger.String("file", filePath),
			logger.Int("attempt", attempt),
			logger.Int("max_attempts", maxRetries))

		// Update upload attempt count
		fileInfo.UploadAttempts = attempt
		updateMetadataFile(metadataPath, fileInfo, "upload_attempted", "")

		err := u.uploader.UploadFile(filePath)
		if err == nil {
			// Upload successful - update metadata and delete file
			logger.Info("File uploaded successfully", logger.String("file", filePath))

			// Generate S3 URL (simplified - in real implementation you'd get this from the upload result)
			filename := filepath.Base(filePath)
			s3URL := fmt.Sprintf("https://%s.s3.amazonaws.com/%s", config.S3.Bucket, filename)

			uploadedAt := time.Now()
			fileInfo.UploadStatus = "uploaded"
			fileInfo.UploadURL = s3URL
			fileInfo.UploadedAt = &uploadedAt
			fileInfo.LastUploadError = ""

			// Update metadata with success
			updateMetadataFile(metadataPath, fileInfo, "upload_completed", s3URL)

			// Delete local file if configured (defaulting to true for now)
			deleteAfterUpload := true // This could be configurable per task
			if deleteAfterUpload {
				if err := os.Remove(filePath); err != nil {
					logger.Warn("Failed to delete file after upload", logger.String("file", filePath), logger.ErrorField(err))
				} else {
					logger.Info("Deleted local file after successful upload", logger.String("file", filePath))
					updateMetadataFile(metadataPath, fileInfo, "file_deleted_after_upload", "")
				}
			}

			return nil
		}

		// Upload failed - update metadata with error
		logger.Warn("Upload attempt failed",
			logger.String("file", filePath),
			logger.Int("attempt", attempt),
			logger.ErrorField(err))
		fileInfo.LastUploadError = err.Error()
		updateMetadataFile(metadataPath, fileInfo, "upload_failed", err.Error())

		// If not the last attempt, wait with exponential backoff
		if attempt < maxRetries {
			delay := time.Duration(attempt) * baseDelay
			logger.Info("Waiting before retry",
				logger.String("file", filePath),
				logger.Duration("delay", delay),
				logger.Int("next_attempt", attempt+1))
			time.Sleep(delay)
		}
	}

	// All retries failed
	fileInfo.UploadStatus = "failed"
	updateMetadataFile(metadataPath, fileInfo, "upload_failed_final", "Max retries exceeded")
	return fmt.Errorf("upload failed after %d attempts", maxRetries)
}

// NewCustomWatcher creates a watcher that uses completion-aware uploading
func NewCustomWatcher(outputDir string, s3Config uploader.S3Config, customUploader *CompletionAwareUploader) (*uploader.Watcher, error) {
	// We'll use the existing watcher structure but need to adapt it
	// For now, let's use the standard watcher - the completion check happens in the uploader
	return uploader.NewWatcher(outputDir, s3Config)
}

func healthCheckHandler(c *gin.Context) {
	// Simple health check using existing watcher information
	status := "healthy"
	if len(watchers) == 0 {
		status = "unhealthy"
	}

	watcherInfo := make([]map[string]string, len(watchers))
	for i := range watchers {
		watcherInfo[i] = map[string]string{
			"directory": config.Watcher.Directories[i],
			"status":    "active",
		}
	}

	c.JSON(http.StatusOK, gin.H{
		"status":            status,
		"service":           "uploader",
		"version":           version.GetVersion(),
		"uptime":            time.Since(startTime).String(),
		"watchers":          watcherInfo,
		"s3_bucket":         config.S3.Bucket,
		"s3_region":         config.S3.Region,
		"watch_directories": config.Watcher.Directories,
	})
}

func startHealthCheckServer() {
	r := gin.New()
	r.Use(logger.GinRequestLogger(), gin.Recovery())

	// Health check
	r.GET("/health", healthCheckHandler)

	// Simple metrics endpoint
	r.GET("/metrics", func(c *gin.Context) {
		metrics := map[string]interface{}{
			"watchers_count": len(watchers),
			"directories":    config.Watcher.Directories,
			"s3_bucket":      config.S3.Bucket,
			"uptime":         time.Since(startTime).String(),
		}
		c.JSON(http.StatusOK, metrics)
	})

	go func() {
		srv := &http.Server{
			Addr:    fmt.Sprintf(":%d", config.Server.HealthPort),
			Handler: r,
		}
		if err := srv.ListenAndServe(); err != nil && err != http.ErrServerClosed {
			logger.Fatal("Failed to start uploader health check server", logger.ErrorField(err))
		}
	}()

	logger.Info("Uploader health server started", logger.Int("port", config.Server.HealthPort))
}

func scanAndUploadExistingFiles() {
	// Periodically scan for newly completed files that might have been missed
	ticker := time.NewTicker(60 * time.Second) // Increased to 60 seconds to be less aggressive
	defer ticker.Stop()

	customUploader, err := NewCompletionAwareUploader(config.S3)
	if err != nil {
		logger.Error("Failed to create uploader for background scanning", logger.ErrorField(err))
		return
	}

	for range ticker.C {
		logger.Info("Scanning for completed files to upload")
		for _, watchDir := range config.Watcher.Directories {
			filepath.Walk(watchDir, func(path string, info os.FileInfo, err error) error {
				if err != nil {
					return nil
				}

				// Skip directories and metadata files
				if info.IsDir() || strings.HasSuffix(path, ".json") {
					return nil
				}

				// Use the completion-aware uploader which handles all the logic
				if err := customUploader.UploadFile(path); err != nil {
					logger.Warn("Background scan failed to process file", logger.String("file", path), logger.ErrorField(err))
				}

				return nil
			})
		}
	}
}

func main() {
	logger.Init("uploader")
	logger.Info("uploader service starting", logger.String("version", version.FullVersion()))

	// Load configuration
	if err := loadConfig(); err != nil {
		logger.Fatal("Failed to load configuration", logger.ErrorField(err))
	}

	// Start health check server
	startHealthCheckServer()

	// Start file watchers (main functionality)
	if err := startFileWatchers(); err != nil {
		logger.Fatal("Failed to start file watchers", logger.ErrorField(err))
	}

	// Start background scanner for missed completed files
	go scanAndUploadExistingFiles()

	logger.Info("Uploader service started",
		logger.String("directories", fmt.Sprint(config.Watcher.Directories)),
		logger.String("s3_bucket", config.S3.Bucket),
		logger.String("s3_region", config.S3.Region),
		logger.Int("health_port", config.Server.HealthPort))

	// Wait for interrupt signal to gracefully shutdown
	quit := make(chan os.Signal, 1)
	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
	<-quit

	logger.Info("Uploader service stopping")

	// The existing watcher package handles graceful shutdown via context cancellation
	logger.Info("Stopped watchers", logger.Int("count", len(watchers)))

	logger.Info("Uploader service stopped")
}
